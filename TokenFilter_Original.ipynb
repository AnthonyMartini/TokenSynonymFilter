{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "token_filter_title",
   "metadata": {},
   "source": [
    "# Token Filter\n",
    "\n",
    "This notebook implements token filtering logic, likely based on embeddings and character validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eabd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip installations\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install accelerate -U\n",
    "%pip install datasets\n",
    "%pip install evaluate\n",
    "%pip install transformers\n",
    "%pip install editdistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eaa4d0",
   "metadata": {},
   "source": [
    "## Step -1: Filter Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4af94",
   "metadata": {},
   "source": [
    "### Import libraries and get embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert_tokenizer_uncased\")\n",
    "model = AutoModel.from_pretrained(\"./models/BaseModel_uncased_H100/checkpoint-11380\")\n",
    "\n",
    "# Get full embedding matrix\n",
    "\n",
    "\n",
    "embedding_matrix = model.get_input_embeddings().weight  # shape: (vocab_size, hidden_size)\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b468d8",
   "metadata": {},
   "source": [
    "## Step 1: Filter Out Irrelevant Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05805b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 4981 tokens are unused or single characters\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "vocab_tokens = [(i,tokenizer.decode([i])) for i in range(4143,vocab_size)]\n",
    "print(vocab_tokens)\n",
    "\n",
    "#filter out any tokens with numbers in them\n",
    "filtered_tokens  = torch.tensor([idx for idx, token in vocab_tokens if not (any((char.isdigit() or char == '#')  for char in token) or len(token) < 3)])\n",
    "print(\"Vocab size after removing tokens with numbers:\", len(filtered_tokens))\n",
    "print(\"Vocab size after removing single char tokens:\", len(vocab_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9e6efc",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = embedding_matrix[filtered_tokens].to(device)\n",
    "\n",
    "# 1. Normalize embeddings to unit vectors\n",
    "norm_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# 2. Calculate the full Similarity Matrix\n",
    "# Result is (N, N). For 20k tokens, this is ~800MB in float16\n",
    "sim_matrix = torch.mm(norm_embeddings, norm_embeddings.t())\n",
    "\n",
    "# 3. Mask the diagonal (Self-similarity is always 1.0)\n",
    "n = sim_matrix.size(0)\n",
    "diag_indices = torch.arange(n, device=sim_matrix.device)\n",
    "sim_matrix[diag_indices, diag_indices] = -1.0  # Set to -1 so topk ignores them\n",
    "\n",
    "# 4. Get the Top 40 closest neighbors for every single token\n",
    "k_neighbors = 20\n",
    "values, indices = torch.topk(sim_matrix, k=k_neighbors, largest=True, dim=1)\n",
    "\n",
    "# 5. Find the 100 \"Closest\" pairs globally across the entire matrix\n",
    "k_global = 10000\n",
    "flat_values = values.view(-1)\n",
    "print(\"Flat values shape:\", flat_values.shape)\n",
    "global_max_vals, global_max_idxs = torch.topk(flat_values, k=k_global * 2, largest=True)\n",
    "\n",
    "# 6. Map back to token indices\n",
    "# row_idx: The source word\n",
    "# neighbor_idx: The similar word\n",
    "row_indices = global_max_idxs // k_neighbors\n",
    "neighbor_indices = indices.view(-1)[global_max_idxs]\n",
    "\n",
    "\n",
    "print(\"global max vals shape:\", global_max_vals.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e46f9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.55\n",
    "epsilon = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f4756",
   "metadata": {},
   "source": [
    "## Filter Pairs Based on Filter Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e776be",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_pairs = set()\n",
    "\n",
    "for i in range(len(global_max_vals)):\n",
    "    u, v = row_indices[i].item(), neighbor_indices[i].item()\n",
    "\n",
    "    word1 = tokenizer.decode([filtered_tokens[u].item()])\n",
    "    word2 = tokenizer.decode([filtered_tokens[v].item()])\n",
    "    score = global_max_vals[i].item()\n",
    "    edit_d_ratio = editdistance.eval(word1, word2) / max(len(word1), len(word2))\n",
    "\n",
    "    #If the edit distance ratio is greater than epsilon and the score is greater than theta, add to seen pairs\n",
    "    if edit_d_ratio > epsilon and score >= theta:\n",
    "        pair = tuple((tuple(sorted((word1, word2))),score))\n",
    "        \n",
    "        if pair not in seen_pairs:\n",
    "            seen_pairs.add(pair)\n",
    "\n",
    "print(\"Number of pairs \", len(seen_pairs))\n",
    "print(seen_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98474416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Number of pairs after edit distance filtering: {len(seen_pairs)}\")\n",
    "print(f\"{'Word 1':<20} | {'Word 2':<20} | {'Cosine Sim':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for pair in seen_pairs:\n",
    "    word1 = pair[0][0]\n",
    "    word2 = pair[0][1]\n",
    "    score = pair[1]\n",
    "\n",
    "    print(f\"{word1:<20} | {word2:<20} | {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a97011",
   "metadata": {},
   "source": [
    "## Step 4: Create Counts of Each Token's Occurance and Nearby Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e562a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in filtered pairs: 4352\n"
     ]
    }
   ],
   "source": [
    "unique_words = dict()\n",
    "unique_words_related = {}\n",
    "for pair in seen_pairs:\n",
    "    unique_words[pair[0][0]] = 0\n",
    "    unique_words[pair[0][1]] = 0\n",
    "    unique_words_related[pair[0][0]] = set()\n",
    "    unique_words_related[pair[0][1]] = set()\n",
    "print(f\"Number of unique words in filtered pairs: {len(unique_words)}\")\n",
    "\n",
    "for pair in seen_pairs:\n",
    "    unique_words[pair[0][0]] += 1\n",
    "    unique_words[pair[0][1]] += 1\n",
    "    unique_words_related[pair[0][0]].add(pair[0][1])\n",
    "    unique_words_related[pair[0][1]].add(pair[0][0])\n",
    "\n",
    "\n",
    "sorted_unique_words = sorted(unique_words.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_words_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1ec5cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Your data: { ( (w1, w2), score ), ... }\n",
    "data = seen_pairs \n",
    "\n",
    "with open(\"manual_review.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Word 1\", \"Word 2\", \"Score\"])\n",
    "    \n",
    "    # Unpack the nested structure: ((w1, w2), score)\n",
    "    for (words, score) in data:\n",
    "        word1, word2 = words\n",
    "        writer.writerow([word1, word2, score])\n",
    "\n",
    "print(\"CSV saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the words withthe highest counts\n",
    "\n",
    "print(f\"{'Word':<20} | {'Count':<10} | {'Related Words'}\")\n",
    "print(\"-\" * 60)\n",
    "for word, count in sorted_unique_words:\n",
    "    related_words = \", \".join(sorted(unique_words_related[word]))\n",
    "    print(f\"{word:<20} | {count:<10} | {related_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c7fc0",
   "metadata": {},
   "source": [
    "## Step 5: Iterate Through Sorted Frequencies and Remove Related Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1225b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the most common words and remove all related words\n",
    "\n",
    "mapping = dict()\n",
    "\n",
    "removed_words = set()\n",
    "while len(sorted_unique_words) > 0:\n",
    "    \n",
    "    word, count = sorted_unique_words[0]\n",
    "    if count == 0:\n",
    "        break\n",
    "    if word not in unique_words:\n",
    "        continue\n",
    "\n",
    "    #Pop the top word from the list\n",
    "    unique_words.pop(word, None)\n",
    "    related_words = set(unique_words_related[word])\n",
    "\n",
    "    # Remove related words from all other entries\n",
    "    for related_word in related_words:\n",
    "        mapping[related_word] = word\n",
    "        removed_words.add(related_word)\n",
    "        #remove the keys for the related words\n",
    "        unique_words.pop(related_word, None)\n",
    "        unique_words_related.pop(related_word, None)\n",
    "        #remove the instance of the related word from all other related word sets\n",
    "        for word2 in unique_words:\n",
    "            if related_word in unique_words_related[word2]:\n",
    "                unique_words_related[word2].remove(related_word)\n",
    "                unique_words[word2] -= 1\n",
    "    sorted_unique_words = sorted(unique_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "\n",
    "print(f\"Number of removed words: {len(removed_words)}\")\n",
    "print(f\"Removed words: {', '.join(sorted(removed_words))}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8da4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a1268",
   "metadata": {},
   "source": [
    "## Step 6: Remove Words and Save Tokenizer Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = tokenizer.get_vocab()\n",
    "for word in removed_words:\n",
    "    model_state.pop(word, None)\n",
    "\n",
    "print(model_state)\n",
    "vocab_list = [token for token, idx in sorted(model_state.items(), key=lambda x: x[1])]\n",
    "print(vocab_list)\n",
    "print(len(vocab_list), \"words in new vocab\")\n",
    "\n",
    "import json\n",
    "with open(\"filtered_tokenizer_vocab_055_0.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "with open(\"removed_words_mapping_055_0.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
