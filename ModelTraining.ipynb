{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7720d0",
   "metadata": {},
   "source": [
    "# Pre-process Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c53faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def is_chinese_char(cp):\n",
    "    \"\"\"Checks whether CP is the codepoint of a CJK character based on BERT rules.\"\"\"\n",
    "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  \n",
    "            (cp >= 0x3400 and cp <= 0x4DBF) or  \n",
    "            (cp >= 0x20000 and cp <= 0x2A6DF) or  \n",
    "            (cp >= 0x2A700 and cp <= 0x2B73F) or  \n",
    "            (cp >= 0x2B740 and cp <= 0x2B81F) or  \n",
    "            (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "            (cp >= 0xF900 and cp <= 0xFAFF) or  \n",
    "            (cp >= 0x2F800 and cp <= 0x2FA1F)):  \n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean_and_save(input_path, output_path):\n",
    "    print(\"Cleaning corpus with CJK handling...\")\n",
    "    with open(input_path, 'r', encoding='utf-8') as f_in, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            # 1. CJK Spacing: Wrap Chinese chars in spaces\n",
    "            chars = []\n",
    "            for char in line:\n",
    "                cp = ord(char)\n",
    "                if is_chinese_char(cp):\n",
    "                    chars.append(f\" {char} \")\n",
    "                else:\n",
    "                    chars.append(char)\n",
    "            line = \"\".join(chars)\n",
    "\n",
    "            # 2. Normalize Unicode (NFD)\n",
    "            line = unicodedata.normalize('NFD', line)\n",
    "            \n",
    "            # 3. Strip Accents\n",
    "            line = \"\".join([c for c in line if not unicodedata.combining(c)])\n",
    "            \n",
    "            # 4. Lowercase and clean up resulting double-spaces\n",
    "            # Using .split() and .join() keeps exactly one space between words\n",
    "            final_line = \" \".join(line.lower().split())\n",
    "            f_out.write(final_line + \"\\n\")\n",
    "\n",
    "    print(\"Clean corpus saved! Ready for training.\")\n",
    "\n",
    "clean_and_save(\"./corpus/wiki_corpus.txt\", \"./corpus/wiki_corpus_clean.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a22f54",
   "metadata": {},
   "source": [
    "### Mount for Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8510f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# 1. Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "source_path = '/content/drive/MyDrive/Colab Notebooks/TokenFilter'\n",
    "\n",
    "if os.path.exists(source_path):\n",
    "    # This command copies the *contents* of TokenFilter to the current folder (.)\n",
    "    !cp -r \"{source_path}\"/* .\n",
    "    print(\"Success! All files copied.\")\n",
    "else:\n",
    "    print(f\"Error: Could not find path {source_path}. Check if the folder name is correct.\")\n",
    "\n",
    "# 5. Create any output directories your code expects if they weren't in the copy\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"bert_tokenizer_uncased\", exist_ok=True)\n",
    "os.makedirs(\"filtered_bert_tokenizer\", exist_ok=True)\n",
    "\n",
    "print(\"Copy complete! Directory structure:\")\n",
    "!ls -R ./models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5133e1a8",
   "metadata": {},
   "source": [
    "## Generate Word-Piece Tokenizer From Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antho/TokenSynonymFilter/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer...\n",
      "Training tokenizer...\n",
      "\n",
      "\n",
      "\n",
      "Tokenizer vocab size: 30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert_tokenizer_uncased/tokenizer_config.json',\n",
       " './bert_tokenizer_uncased/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Configure trainer\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=special_tokens\n",
    "\n",
    ")\n",
    "\n",
    "# Train tokenizer\n",
    "print(\"Training tokenizer...\")\n",
    "\n",
    "# Pass the file\n",
    "tokenizer.train([\"./corpus/wiki_corpus_clean.txt\"], trainer)\n",
    "\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=True,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "# Save for reuse like AutoTokenizer\n",
    "\n",
    "tokenizer.save_pretrained(\"./bert_tokenizer_uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c00e8",
   "metadata": {},
   "source": [
    "## Load Saved Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a4fe9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./filtered_bert_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1050a43",
   "metadata": {},
   "source": [
    "## Generate Word-Piece Tokenizer From JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beea469",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokenizer_path = \"filtered_tokenizer_vocab_06_05.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 29709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./filtered_bert_tokenizer/tokenizer_config.json',\n",
       " './filtered_bert_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load filtered vocab\n",
    "\n",
    "with open(filtered_tokenizer_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    token_list = json.load(f)  \n",
    "\n",
    "# Convert list to dict {token: id}\n",
    "vocab_dict = {token: idx for idx, token in enumerate(token_list)}\n",
    "\n",
    "# Initialize WordPiece tokenizer with your vocab\n",
    "tokenizer = Tokenizer(WordPiece(vocab=vocab_dict, unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Wrap in PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=True,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "\n",
    "tokenizer.save_pretrained(\"./filtered_bert_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1ea41",
   "metadata": {},
   "source": [
    "### Testing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6da22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12823]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"vertical\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ae7c4",
   "metadata": {},
   "source": [
    "# Define The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# new Model config\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=256, # Increased to 128 (64 is too short for real sentences)\n",
    "    num_hidden_layers=8,        # 2 is too shallow; 4 allows abstraction\n",
    "    num_attention_heads=8,      # More heads = better context understanding\n",
    "    hidden_size=768,            # 64 -> 256 (Crucial for representational capacity)\n",
    "    intermediate_size=3072,     # usually 4x hidden_size (Standard FFN ratio)\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7afd3",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72284acd",
   "metadata": {},
   "source": [
    "After filtering tokens, we will define a pattern to map the corpus to replacement words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4607efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open(\"mapping_06_05.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_map = json.load(f)\n",
    "pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, word_map.keys())) + r')\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961d0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = None #reset pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc902750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1165029 examples [00:05, 212692.39 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 1165029/1165029 [00:54<00:00, 21502.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\"train\": \"./corpus/wiki_corpus_clean.txt\"},\n",
    ")\n",
    "def replace_tokens(text):\n",
    "    # Check if 'pattern' exists in the namespace and is not None\n",
    "    if 'pattern' not in globals() or pattern is None:\n",
    "        return text\n",
    "    \n",
    "    return pattern.sub(lambda x: word_map[x.group(0)], text)\n",
    "\n",
    "def tokenize(batch):\n",
    "    processed_text = [replace_tokens(t) for t in batch[\"text\"]]\n",
    "    return tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9508c067",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(config) #Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573878d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compiled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='27306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    6/27306 00:00 < 1:32:17, 4.93 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel Compiled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Capture the output of the train() method\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m train_results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Explicitly print the results object\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Training Summary ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TokenSynonymFilter/venv/lib/python3.12/site-packages/transformers/trainer.py:2170\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2168\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2171\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TokenSynonymFilter/venv/lib/python3.12/site-packages/transformers/trainer.py:2542\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2536\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2537\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2540\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2541\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2543\u001b[39m ):\n\u001b[32m   2544\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2545\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2546\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./H100_BERT_Run\",\n",
    "    per_device_train_batch_size=512,  \n",
    "    gradient_accumulation_steps=1, \n",
    "    learning_rate=1e-3,                \n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=5,                \n",
    "    warmup_steps=500,                  \n",
    "    \n",
    "    bf16=True,                         \n",
    "    tf32=True,                         \n",
    "    torch_compile=True,              \n",
    "    optim=\"adamw_torch_fused\",        \n",
    "    dataloader_num_workers=8,          #Test this\n",
    "    dataloader_pin_memory=True,      \n",
    "    \n",
    "    # --- Logging & Saving ---\n",
    "    logging_steps=100,                \n",
    "    save_steps=10000,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"Model Compiled\")\n",
    "train_results = trainer.train()\n",
    "\n",
    "# Explicitly print the results object\n",
    "print(\"\\n--- Training Summary ---\")\n",
    "print(train_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df49f4",
   "metadata": {},
   "source": [
    "# Optional: Test Mask Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834dfe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 106/106 [00:00<00:00, 685.04it/s, Materializing param=cls.predictions.transform.dense.weight]               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.14827008545398712,\n",
       "  'token': 5776,\n",
       "  'token_str': 'city',\n",
       "  'sequence': 'the capital of france is the center of the city .'},\n",
       " {'score': 0.12068881094455719,\n",
       "  'token': 7861,\n",
       "  'token_str': 'capital',\n",
       "  'sequence': 'the capital of france is the center of the capital .'},\n",
       " {'score': 0.057559866458177567,\n",
       "  'token': 9659,\n",
       "  'token_str': 'economy',\n",
       "  'sequence': 'the capital of france is the center of the economy .'},\n",
       " {'score': 0.0510672926902771,\n",
       "  'token': 7872,\n",
       "  'token_str': 'empire',\n",
       "  'sequence': 'the capital of france is the center of the empire .'},\n",
       " {'score': 0.045801009982824326,\n",
       "  'token': 8873,\n",
       "  'token_str': 'republic',\n",
       "  'sequence': 'the capital of france is the center of the republic .'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, pipeline\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"./models/BaseModel/checkpoint-27306/\")\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "fill_mask(\"The capital of france is the center of the [MASK] .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b11a8",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26aeffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer)\n",
    "\n",
    "model_to_fine_tune_path = \"./models/06_05/checkpoint-27306\"\n",
    "fine_tuned_model_path = \"./models/06_05_Classification\"\n",
    "tokenizer_path = \"./filtered_bert_tokenizer_06_05\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f095f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open(\"mapping_06_05.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_map = json.load(f)\n",
    "pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, word_map.keys())) + r')\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = None #No pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67386e3",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:16<00:00, 1523.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "        DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# Load IMDB data\n",
    "dataset = load_dataset('csv', data_files={\n",
    "    'train': './corpus/imdb_train.csv',\n",
    "})\n",
    "def replace_tokens(text):\n",
    "    # Check if 'pattern' exists in the namespace and is not None\n",
    "    if 'pattern' not in globals() or pattern is None:\n",
    "        return text\n",
    "    \n",
    "    # If it exists, perform the replacement\n",
    "    return pattern.sub(lambda x: word_map[x.group(0)], text)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    processed_text = [replace_tokens(t) for t in examples[\"text\"]]\n",
    "    return tokenizer(\n",
    "        processed_text, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f437b1",
   "metadata": {},
   "source": [
    "## Fine tune Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64d6d77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 101/101 [00:00<00:00, 631.38it/s, Materializing param=bert.encoder.layer.5.output.dense.weight]             \n",
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: ./models/06_05/checkpoint-27306\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "classifier.bias                            | MISSING    | \n",
      "bert.pooler.dense.weight                   | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fine-Tuning for Classification...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7820' max='7820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7820/7820 06:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.577089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.449693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.394305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.349170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.320376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.289575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.257898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.234887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.198244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.182617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.169885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.146572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.131644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.128205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.115770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Summary ---\n",
      "TrainOutput(global_step=7820, training_loss=0.25697133730134697, metrics={'train_runtime': 383.6352, 'train_samples_per_second': 651.661, 'train_steps_per_second': 20.384, 'total_flos': 3682369920000000.0, 'train_loss': 0.25697133730134697, 'epoch': 10.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_to_fine_tune_path,  #Model to be fine-tuned\n",
    "    num_labels=2  # e.g., Positive and Negative\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./temp_dir\",      # A path is still required by the API\n",
    "    \n",
    "    learning_rate=2e-5,               # Small LR to preserve pre-trained knowledge\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,                       \n",
    "    torch_compile=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting Fine-Tuning for Classification...\")\n",
    "# Capture the output of the train() method\n",
    "train_results = trainer.train()\n",
    "\n",
    "print(\"\\n--- Training Summary ---\")\n",
    "print(train_results)\n",
    "\n",
    "\n",
    "# Save the final classification model\n",
    "model.save_pretrained(fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b989eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 105/105 [00:00<00:00, 612.49it/s, Materializing param=classifier.weight]                                    \n",
      "Map: 100%|██████████| 25000/25000 [00:12<00:00, 2003.54 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on test data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Metrics ---\n",
      "Accuracy:  0.8283\n",
      "Precision: 0.8055\n",
      "F1 Score:  0.8344\n",
      "Recall:    0.8655\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "model_path = fine_tuned_model_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'test': './corpus/imdb_test.csv'})\n",
    "\n",
    "def replace_tokens(text):\n",
    "    # Check if 'pattern' exists in the namespace and is not None\n",
    "    if 'pattern' not in globals() or pattern is None:\n",
    "        return text\n",
    "    \n",
    "    # If it exists, perform the replacement\n",
    "    return pattern.sub(lambda x: word_map[x.group(0)], text)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    processed_text = [replace_tokens(t) for t in examples[\"text\"]]\n",
    "    return tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_test = dataset['test'].map(tokenize_function, batched=True)\n",
    "metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "eval_args = TrainingArguments(\n",
    "    per_device_eval_batch_size=32,\n",
    "    bf16=True,            \n",
    "    torch_compile=True,    \n",
    "    report_to=\"none\"      \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    train_dataset=None,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Running evaluation on test data...\")\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "print(f\"Accuracy:  {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Precision: {results['eval_precision']:.4f}\")\n",
    "print(f\"F1 Score:  {results['eval_f1']:.4f}\")\n",
    "print(f\"Recall:    {results['eval_recall']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
