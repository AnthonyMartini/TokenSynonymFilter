{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5133e1a8",
   "metadata": {},
   "source": [
    "## Create Word-Piece Tokenizer From Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c54a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer...\n",
      "Training tokenizer...\n",
      "Tokenizer vocab size: 30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert_tokenizer\\\\tokenizer_config.json',\n",
       " './bert_tokenizer\\\\special_tokens_map.json',\n",
       " './bert_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Configure trainer\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "\n",
    "# Train tokenizer\n",
    "print(\"Training tokenizer...\")\n",
    "corpus_file = \"./corpus/wiki_corpus.txt\"\n",
    "tokenizer.train([corpus_file], trainer)\n",
    "\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "# Save for reuse like AutoTokenizer\n",
    "\n",
    "tokenizer.save_pretrained(\"./bert_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1050a43",
   "metadata": {},
   "source": [
    "## Create Word-Piece Tokenizer From JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load filtered vocab\n",
    "\n",
    "with open(\"filtered_tokenizer_vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    token_list = json.load(f)  \n",
    "\n",
    "# Convert list to dict {token: id}\n",
    "vocab_dict = {token: idx for idx, token in enumerate(token_list)}\n",
    "\n",
    "# Initialize WordPiece tokenizer with your vocab\n",
    "tokenizer = Tokenizer(WordPiece(vocab=vocab_dict, unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Optional: CLS/SEP post-processing (like BERT)\n",
    "tokenizer.post_processor = BertProcessing(\n",
    "    (\"[SEP]\", vocab_dict[\"[SEP]\"]),\n",
    "    (\"[CLS]\", vocab_dict[\"[CLS]\"])\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap in PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Save for reuse like AutoTokenizer\n",
    "# -----------------------------\n",
    "tokenizer.save_pretrained(\"./filtered_bert_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1c6da22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17278]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"vertical\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "#find out how to load tokenizer - \n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"distilbert_tokenizer.json\", \n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c395bb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c5dfc0730145b99582e21c79f7795f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1165029 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "\n",
    "# Model config\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=64,  # small for testing\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=2,\n",
    "    hidden_size=32,\n",
    "    intermediate_size=64,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "# Dataset\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\"train\": \"./corpus/wiki_corpus.txt\"},\n",
    ")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=16,\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "# Data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9508c067",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "573878d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea6bc5271b747d9a7d44a4bfe44f2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9841, 'grad_norm': 0.7625142335891724, 'learning_rate': 0.0001097694840834248, 'epoch': 0.02}\n",
      "{'loss': 7.0163, 'grad_norm': 1.5524271726608276, 'learning_rate': 0.0002195389681668496, 'epoch': 0.04}\n",
      "{'loss': 7.0367, 'grad_norm': 0.9017856121063232, 'learning_rate': 0.00032930845225027445, 'epoch': 0.07}\n",
      "{'loss': 7.0043, 'grad_norm': 1.2243329286575317, 'learning_rate': 0.0004390779363336992, 'epoch': 0.09}\n",
      "{'loss': 6.9634, 'grad_norm': 1.3926067352294922, 'learning_rate': 0.0004945672079111219, 'epoch': 0.11}\n",
      "{'loss': 6.9214, 'grad_norm': 1.185500144958496, 'learning_rate': 0.00048235868636308145, 'epoch': 0.13}\n",
      "{'loss': 6.9069, 'grad_norm': 1.281945824623108, 'learning_rate': 0.0004701501648150409, 'epoch': 0.15}\n",
      "{'loss': 6.8767, 'grad_norm': 1.4221779108047485, 'learning_rate': 0.0004579416432670004, 'epoch': 0.18}\n",
      "{'loss': 6.847, 'grad_norm': 1.5178356170654297, 'learning_rate': 0.00044573312171895986, 'epoch': 0.2}\n",
      "{'loss': 6.8182, 'grad_norm': 1.6842323541641235, 'learning_rate': 0.00043352460017091933, 'epoch': 0.22}\n",
      "{'loss': 6.8041, 'grad_norm': 3.4094488620758057, 'learning_rate': 0.0004213160786228788, 'epoch': 0.24}\n",
      "{'loss': 6.7956, 'grad_norm': 2.8161814212799072, 'learning_rate': 0.00040910755707483827, 'epoch': 0.26}\n",
      "{'loss': 6.7703, 'grad_norm': 2.139131784439087, 'learning_rate': 0.00039689903552679774, 'epoch': 0.29}\n",
      "{'loss': 6.7356, 'grad_norm': 1.846503734588623, 'learning_rate': 0.0003847515565864974, 'epoch': 0.31}\n",
      "{'loss': 6.7223, 'grad_norm': 2.5980629920959473, 'learning_rate': 0.0003725430350384568, 'epoch': 0.33}\n",
      "{'loss': 6.6767, 'grad_norm': 1.977245569229126, 'learning_rate': 0.0003603345134904163, 'epoch': 0.35}\n",
      "{'loss': 6.6748, 'grad_norm': 1.838828444480896, 'learning_rate': 0.00034812599194237575, 'epoch': 0.37}\n",
      "{'loss': 6.6335, 'grad_norm': 2.0313339233398438, 'learning_rate': 0.0003359174703943352, 'epoch': 0.4}\n",
      "{'loss': 6.6314, 'grad_norm': 2.6123862266540527, 'learning_rate': 0.0003237089488462947, 'epoch': 0.42}\n",
      "{'loss': 6.6092, 'grad_norm': 2.3150382041931152, 'learning_rate': 0.00031150042729825416, 'epoch': 0.44}\n",
      "{'loss': 6.6316, 'grad_norm': 2.426826238632202, 'learning_rate': 0.00029929190575021363, 'epoch': 0.46}\n",
      "{'loss': 6.6326, 'grad_norm': 2.2662627696990967, 'learning_rate': 0.0002870833842021731, 'epoch': 0.48}\n",
      "{'loss': 6.5791, 'grad_norm': 3.217357635498047, 'learning_rate': 0.0002748748626541326, 'epoch': 0.51}\n",
      "{'loss': 6.5744, 'grad_norm': 2.2270257472991943, 'learning_rate': 0.0002627273837138323, 'epoch': 0.53}\n",
      "{'loss': 6.5422, 'grad_norm': 2.5082998275756836, 'learning_rate': 0.00025051886216579175, 'epoch': 0.55}\n",
      "{'loss': 6.4937, 'grad_norm': 2.393406629562378, 'learning_rate': 0.0002383103406177512, 'epoch': 0.57}\n",
      "{'loss': 6.4816, 'grad_norm': 2.122551918029785, 'learning_rate': 0.00022610181906971067, 'epoch': 0.59}\n",
      "{'loss': 6.4481, 'grad_norm': 2.241154670715332, 'learning_rate': 0.00021389329752167014, 'epoch': 0.62}\n",
      "{'loss': 6.4721, 'grad_norm': 2.6432015895843506, 'learning_rate': 0.0002016847759736296, 'epoch': 0.64}\n",
      "{'loss': 6.4475, 'grad_norm': 3.6469173431396484, 'learning_rate': 0.00018947625442558908, 'epoch': 0.66}\n",
      "{'loss': 6.4525, 'grad_norm': 3.2743747234344482, 'learning_rate': 0.00017726773287754852, 'epoch': 0.68}\n",
      "{'loss': 6.3942, 'grad_norm': 2.567640781402588, 'learning_rate': 0.000165059211329508, 'epoch': 0.7}\n",
      "{'loss': 6.4214, 'grad_norm': 2.367699384689331, 'learning_rate': 0.00015285068978146746, 'epoch': 0.73}\n",
      "{'loss': 6.402, 'grad_norm': 2.4611361026763916, 'learning_rate': 0.00014070321084116714, 'epoch': 0.75}\n",
      "{'loss': 6.3934, 'grad_norm': 2.9622128009796143, 'learning_rate': 0.0001284946892931266, 'epoch': 0.77}\n",
      "{'loss': 6.3635, 'grad_norm': 3.0139496326446533, 'learning_rate': 0.00011628616774508607, 'epoch': 0.79}\n",
      "{'loss': 6.382, 'grad_norm': 2.726672410964966, 'learning_rate': 0.00010407764619704554, 'epoch': 0.81}\n",
      "{'loss': 6.3694, 'grad_norm': 2.9432878494262695, 'learning_rate': 9.1869124649005e-05, 'epoch': 0.83}\n",
      "{'loss': 6.3651, 'grad_norm': 3.0544748306274414, 'learning_rate': 7.966060310096447e-05, 'epoch': 0.86}\n",
      "{'loss': 6.3602, 'grad_norm': 2.8801989555358887, 'learning_rate': 6.745208155292394e-05, 'epoch': 0.88}\n",
      "{'loss': 6.3338, 'grad_norm': 2.5637009143829346, 'learning_rate': 5.5243560004883414e-05, 'epoch': 0.9}\n",
      "{'loss': 6.3351, 'grad_norm': 2.9649851322174072, 'learning_rate': 4.303503845684288e-05, 'epoch': 0.92}\n",
      "{'loss': 6.3689, 'grad_norm': 2.598576545715332, 'learning_rate': 3.082651690880235e-05, 'epoch': 0.94}\n",
      "{'loss': 6.3378, 'grad_norm': 2.7070047855377197, 'learning_rate': 1.8679037968502015e-05, 'epoch': 0.97}\n",
      "{'loss': 6.3451, 'grad_norm': 3.040091037750244, 'learning_rate': 6.470516420461482e-06, 'epoch': 0.99}\n",
      "{'train_runtime': 250.7406, 'train_samples_per_second': 4646.352, 'train_steps_per_second': 36.3, 'train_loss': 6.604493662750975, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9102, training_loss=6.604493662750975, metrics={'train_runtime': 250.7406, 'train_samples_per_second': 4646.352, 'train_steps_per_second': 36.3, 'total_flos': 5398874869248.0, 'train_loss': 6.604493662750975, 'epoch': 1.0})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mini_bert\",\n",
    "    per_device_train_batch_size=128,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "834dfe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05591578781604767,\n",
       "  'token': 8680,\n",
       "  'token_str': 'the',\n",
       "  'sequence': 'The dog likes to run around the and also likes to bite'},\n",
       " {'score': 0.03488413617014885,\n",
       "  'token': 69,\n",
       "  'token_str': 'a',\n",
       "  'sequence': 'The dog likes to run around a and also likes to bite'},\n",
       " {'score': 0.012131385505199432,\n",
       "  'token': 16,\n",
       "  'token_str': ',',\n",
       "  'sequence': 'The dog likes to run around , and also likes to bite'},\n",
       " {'score': 0.010184370912611485,\n",
       "  'token': 8700,\n",
       "  'token_str': 'to',\n",
       "  'sequence': 'The dog likes to run around to and also likes to bite'},\n",
       " {'score': 0.010054562240839005,\n",
       "  'token': 8944,\n",
       "  'token_str': 'been',\n",
       "  'sequence': 'The dog likes to run around been and also likes to bite'}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, PreTrainedTokenizerFast, pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Load trained model\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"./mini_bert/checkpoint-9102/\")\n",
    "\n",
    "# Masked language modeling pipeline\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "fill_mask(\"The dog likes to run around [MASK] and also likes to bite \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
